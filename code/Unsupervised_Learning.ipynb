{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#K-means Clustering in Python\n",
        "\n",
        "`KMeans(n_clusters=2)` initializes a k-means model with 2 clusters. Parameters are in scikit-learn docs. Fit the model using `model.fit(df)`, where df is a dataframe.  \n",
        "\n",
        "Commonly used attributes are:\n",
        "\n",
        "* **cluster_centers_** -- finds the centroid of each cluster\n",
        "* **labels_** -- finds the labels of all instances\n",
        "* **inertia_** -- finds the within-cluster sum of squares for the entire dataset\n",
        "\n",
        "The Python code below clusters eruptions at Old Faithful based on the waiting time between eruptions and the duration of each eruption."
      ],
      "metadata": {
        "id": "kgrcUtIjjPW3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-UL-LaVjOLR"
      },
      "outputs": [],
      "source": [
        "# Import packages and functions\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "geyser = pd.read_csv('https://raw.githubusercontent.com/mh2t/CS6140/main/data/oldfaithful.csv')\n",
        "geyser"
      ],
      "metadata": {
        "id": "NFEhzcEejsQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual exploration\n",
        "p = sns.scatterplot(data=geyser, x='Eruption', y='Waiting')\n",
        "p.set_xlabel('Eruption time (min)', fontsize=14)\n",
        "p.set_ylabel('Waiting time (min)', fontsize=14)"
      ],
      "metadata": {
        "id": "UHHG5h06juja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a k-means model with k=2\n",
        "kmModel = KMeans(n_clusters=2)\n",
        "\n",
        "# Fit the model\n",
        "kmModel = kmModel.fit(geyser)\n",
        "\n",
        "# Save the cluster centroids\n",
        "centroids = kmModel.cluster_centers_\n",
        "centroids[1]"
      ],
      "metadata": {
        "id": "c6cHDPHPjv7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the cluster assignments\n",
        "clusters = kmModel.fit_predict(geyser[['Eruption', 'Waiting']])\n",
        "\n",
        "# View the clusters for the first five instances\n",
        "clusters[0:5]"
      ],
      "metadata": {
        "id": "SRGgzNs7jxjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot clusters\n",
        "p = sns.scatterplot(\n",
        "    data=geyser, x='Eruption', y='Waiting', hue=clusters, style=clusters\n",
        ")\n",
        "p.set_xlabel('Eruption time (min)', fontsize=14)\n",
        "p.set_ylabel('Waiting time (min)', fontsize=14)\n",
        "\n",
        "# Add centroid for cluster 0\n",
        "plt.scatter(x=centroids[0, 0], y=centroids[0, 1], c='black')\n",
        "\n",
        "# Add centroid for cluster 1\n",
        "plt.scatter(x=centroids[1, 0], y=centroids[1, 1], c='black', marker='X')"
      ],
      "metadata": {
        "id": "pze2S9iojzFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit k-means clustering with k=1,...,5 and save WCSS for each\n",
        "WCSS = []\n",
        "k = [1, 2, 3, 4, 5]\n",
        "for j in k:\n",
        "    kmModel = KMeans(n_clusters=j)\n",
        "    kmModel = kmModel.fit(geyser)\n",
        "    WCSS.append(kmModel.inertia_)"
      ],
      "metadata": {
        "id": "NHTD1sVqj0or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the WCSS for each cluster\n",
        "ax = plt.figure().gca()\n",
        "plt.plot(k, WCSS, '*-')\n",
        "plt.xlabel('Number of clusters (k)', fontsize=14)\n",
        "plt.ylabel('Within-cluster sum of squares (WCSS)', fontsize=14)"
      ],
      "metadata": {
        "id": "s2MEhobXj19g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Agglomerative Clustering in Python\n",
        "\n",
        "`linkage()` in agglomerative clustering connects data. Key parameters are method (e.g., single, complete) for similarity and metric (e.g., Euclidean) for instance distance. More details are in scipy's hierarchical clustering [docs](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html). `dendrogram()` makes dendrograms from dataframes; [scipy's guide](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html) covers its parameters.  \n",
        "\n",
        "For clustering, distance matrices can be useful. Agglomerative clustering accepts them using `squareform` from `spatial.distance` to input a distance matrix.  \n",
        "\n",
        "The Python code below uses agglomerative clustering to cluster species based on differences in the cytochrome c protein."
      ],
      "metadata": {
        "id": "BbanBNQpj-Ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages and functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from scipy.spatial.distance import squareform\n",
        "\n",
        "# Load the dataset\n",
        "cytochrome = pd.read_csv('https://raw.githubusercontent.com/mh2t/CS6140/main/data/cytochrome.csv', header=None, usecols=range(1, 14))\n",
        "cytochrome"
      ],
      "metadata": {
        "id": "iBqBAhEkkTcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add labels for each species and save as a data frame\n",
        "species = [\n",
        "    \"Human\",\n",
        "    \"Monkey\",\n",
        "    \"Horse\",\n",
        "    \"Cow\",\n",
        "    \"Dog\",\n",
        "    \"Whale\",\n",
        "    \"Rabbit\",\n",
        "    \"Kangaroo\",\n",
        "    \"Chicken\",\n",
        "    \"Penguin\",\n",
        "    \"Duck\",\n",
        "    \"Turtle\",\n",
        "    \"Frog\",\n",
        "]\n",
        "\n",
        "pd.DataFrame(data=cytochrome.to_numpy(), index=species, columns=species)"
      ],
      "metadata": {
        "id": "Kqo_qxIikVM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format the data as a distance matrix\n",
        "# In this case, the data already represents distance between points (species)\n",
        "differences = squareform(cytochrome)"
      ],
      "metadata": {
        "id": "WSWymJLykWxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a clustering model with single linkage\n",
        "clusterModel = linkage(differences, method='single')\n",
        "\n",
        "# Create the dendrogram\n",
        "dendrogram(clusterModel, labels=species, leaf_rotation=90)\n",
        "\n",
        "# Plot the dendrogram\n",
        "plt.ylabel('Amino acid differences', fontsize=14)\n",
        "plt.yticks(np.arange(0, 11, step=1))\n",
        "plt.xlabel('Species', fontsize=14)\n",
        "plt.title('Single linkage clustering', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4S0n-48DkYED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DBSCAN in Python\n",
        "\n",
        "DBSCAN function runs DBSCAN algorithm on vectors/distances. It needs eps and min_samples; defaults are 0.5 and 5. More parameters/values are in [scikit-learn docs](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html).  \n",
        "\n",
        "For varied feature units, standardize using `StandardScaler` from `sklearn.preprocessing`.  \n",
        "\n",
        "The Python code below uses DBSCAN clustering to model homes based on sales price and square footage."
      ],
      "metadata": {
        "id": "55yxJNvlkadi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages and functions\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from numpy import where\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "homes = pd.read_csv('https://raw.githubusercontent.com/mh2t/CS6140/main/data/homes.csv')\n",
        "\n",
        "homes"
      ],
      "metadata": {
        "id": "MwSTZ2gWkxq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a smaller data frame with two variables: Price and Floor\n",
        "homes_pf = homes[['Price', 'Floor']]\n",
        "homes_pf.describe()"
      ],
      "metadata": {
        "id": "BWqvdTOAky5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a scaler to transform values\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply scaler and view result\n",
        "homes_scaled = pd.DataFrame(scaler.fit_transform(homes_pf), columns=['Price', 'Floor'])\n",
        "homes_scaled.describe()"
      ],
      "metadata": {
        "id": "cbFR-9FEk1Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize DBSCAN model\n",
        "# Setting a large epsilon will cluster all \"middle\" values and detect outliers\n",
        "dbscanModel = DBSCAN(eps=1, min_samples=12)\n",
        "\n",
        "# Fit the model\n",
        "dbscanModel = dbscanModel.fit(homes_scaled)"
      ],
      "metadata": {
        "id": "1NNetuaVk1xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict clusters\n",
        "clusters = dbscanModel.fit_predict(homes_scaled)\n",
        "clusters = pd.Categorical(clusters)\n",
        "clusters"
      ],
      "metadata": {
        "id": "Vp44CBzzk3Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize scaled outliers\n",
        "p = sns.scatterplot(data=homes_scaled, x='Floor', y='Price', hue=clusters)\n",
        "p.set_xlabel('Scaled floor', fontsize=14)\n",
        "p.set_ylabel('Scaled price', fontsize=14)"
      ],
      "metadata": {
        "id": "919y9vLrk4UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Points where the prediction is -1 are considered outliers\n",
        "outliers_scaled = homes_scaled[clusters == -1]\n",
        "outliers_scaled"
      ],
      "metadata": {
        "id": "x8-jEqwWk5kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Outliers on original scale (price and square footage in thousands)\n",
        "outliers_unscaled = homes[clusters == -1]\n",
        "outliers_unscaled"
      ],
      "metadata": {
        "id": "uVkpn5wVk78d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize outliers on original scale\n",
        "p = sns.scatterplot(data=homes, x='Floor', y='Price', hue=clusters)\n",
        "p.set_xlabel('Floors', fontsize=14)\n",
        "p.set_ylabel('Price', fontsize=14)"
      ],
      "metadata": {
        "id": "hFwjsfW-k9cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Factor Analysis in Python  \n",
        "\n",
        "Scikit-learn's `decomposition` offers PCA and FactorAnalysis. Both need n_components; default is input features count. More details in [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA) and [FactorAnalysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html#sklearn.decomposition.FactorAnalysis) docs.  \n",
        "\n",
        "The factor loading matrix can be obtained using the code `pca.components_.T * np.sqrt(pca.explained_variance_)`.  \n",
        "\n",
        "The Python code below applies factor analysis to the rock dataset."
      ],
      "metadata": {
        "id": "pSzt4Tb_lFDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pandas package\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib_inline.backend_inline\n",
        "\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
      ],
      "metadata": {
        "id": "G6EJoNKQlbV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the rock.csv dataset\n",
        "rock = pd.read_csv('https://raw.githubusercontent.com/mh2t/CS6140/main/data/rock.csv')"
      ],
      "metadata": {
        "id": "TfTheRZ3lbzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the rock dataframe\n",
        "rock"
      ],
      "metadata": {
        "id": "KhgC9YaHlc7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the correlation matrix using a heatmap\n",
        "plt.figure(figsize=(4, 4))\n",
        "sns.heatmap(rock.corr(), cmap=\"YlGnBu\", annot=True)"
      ],
      "metadata": {
        "id": "r9uFrADSleOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot using perimeter and area\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.scatter(rock['Perimeter'], rock['Area'])\n",
        "plt.xlabel('Perimeter', fontsize=14)\n",
        "plt.ylabel('Area', fontsize=14)"
      ],
      "metadata": {
        "id": "jcVhAOl8lfj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot with a linear regression line\n",
        "model = st.linregress(rock['Perimeter'], rock['Area'])\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.scatter(rock['Perimeter'], rock['Area'])\n",
        "x = np.linspace(0, 5000, 10000)\n",
        "y = model[0] * x + model[1]\n",
        "plt.plot(x, y, '-r', linewidth=2.5)\n",
        "plt.xlabel('Perimeter', fontsize=14)\n",
        "plt.ylabel('Area', fontsize=14)"
      ],
      "metadata": {
        "id": "6K_zEx06lgw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "rock = pd.DataFrame(\n",
        "    scaler.fit_transform(rock), columns=['Area', 'Perimeter', 'Shape', 'Permeability']\n",
        ")"
      ],
      "metadata": {
        "id": "1Mmo9ttmliMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and fit a PCA model on the rock data\n",
        "pcaModel = PCA(n_components=4)\n",
        "pcaModel.fit(rock)"
      ],
      "metadata": {
        "id": "jn0P1RW0ljV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the components\n",
        "pcaModel.components_"
      ],
      "metadata": {
        "id": "KzbXY8o4llqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the explained variance (eigenvalues)\n",
        "pcaModel.explained_variance_"
      ],
      "metadata": {
        "id": "BaWHqL_clm53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the factor loadings\n",
        "pcaModel.components_.T * np.sqrt(pcaModel.explained_variance_)"
      ],
      "metadata": {
        "id": "87nVFkSqloDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scree plot\n",
        "xint = range(0, 5)\n",
        "plt.xticks(xint)\n",
        "plt.plot([1, 2, 3, 4], pcaModel.explained_variance_, 'b*-')\n",
        "plt.xlabel('Factors', fontsize='14')\n",
        "plt.ylabel('Eigenvalues', fontsize='14')"
      ],
      "metadata": {
        "id": "0YHdUr_slqUB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}